{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.config.list_physical_devices('GPU')\n",
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faec44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "#from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import  confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "#import pydot\n",
    "#import pydot_ng as pydot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pydotplus\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "keras.utils.vis_utils.pydot = pydot\n",
    "plot_model(model, to_file = \"Model.png\",  show_shapes=True,  rankdir='TB',expand_nested= True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1e168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_all_L_128_128.csv\n",
      "(9216,)\n",
      "(9216, 384)\n",
      "(72, 128, 128, 3)\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 6 6 6 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/Train2\")\n",
    "#input_folder_path=os.getcwd()\n",
    "#all= input_folder_path.split('/')\n",
    "filename='combined_all_L_128_128.csv' #'combined_all_partial.csv'#filename='combined_all_1_L_32_32.csv'\n",
    "print(filename)\n",
    "#train_df = pd.read_csv(filename)\n",
    "train_df = pd.read_csv(filename)\n",
    "train_labels1 =  train_df['Label '].values #train_df['Label '].values\n",
    "#train_labels1 = train_df.iloc[0:,0].values\n",
    "#train_labels1 = to_categorical(train_labels1)\n",
    "print(train_labels1.shape)\n",
    "train_images = (train_df.iloc[:,1:].values).astype('float32')/10000.0\n",
    "print(train_images.shape)\n",
    "train_images = train_images.reshape(72,128,128,3)\n",
    "print(train_images.shape)\n",
    "train_labels1 = train_labels1.reshape(72,128,1)\n",
    "train_labels4= np.zeros(72)\n",
    "for i in range(72):\n",
    "  #  for j in range(6):\n",
    "    if(train_labels1[i,0]  > 0):\n",
    "        train_labels4[i] = train_labels1[i,0]\n",
    "train_labels4=train_labels4.astype('int')\n",
    "print(train_labels4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9486e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train111 = np. concatenate((train_images, train_images))\n",
    "labelsss = np. concatenate((train_labels4, train_labels4))\n",
    "#classes=['low','moderate','high','very high']\n",
    "classes=['no carbon','very low', 'low','moderate','high','very high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f120e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 128, 128, 3) (144,)\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 6 6 6 2 2 2 2 2 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 6 6 6 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(train111.shape, labelsss.shape)\n",
    "train_label=train_labels4.reshape(-1,)\n",
    "train_labels=labelsss.reshape(-1,)\n",
    "print(labelsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(x,y,index):\n",
    "    plt.figure(figsize=(15,2))\n",
    "    plt.imshow(x[index])\n",
    "    plt.xlabel(classes[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(train_images,train_label,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b7d1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 128, 128, 3)\n",
      "(115,)\n",
      "(29, 128, 128, 3)\n",
      "(29,)\n"
     ]
    }
   ],
   "source": [
    "#x_train,  x_test, y_train, y_test = train_test_split(train_images,train_label,random_state=2020,test_size=0.2)\n",
    "x_train,  x_test, y_train, y_test = train_test_split(train111,train_labels,random_state=2020,test_size=0.2)\n",
    "#x_train= train_images\n",
    "#y_train =train_label\n",
    "print(x_train.shape)\n",
    "#print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "#x_test=train_images\n",
    "#y_test = train_label\n",
    "#x_train= train_images\n",
    "#y_train =train_label\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.metrics\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=16,kernel_size=(3,3), strides=(2,2), input_shape=(32,32,3), data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "  #  keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "   # keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.metrics\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=16,kernel_size=(3,3), strides=(2,2), input_shape=(64,64,3), data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "  #  keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "   # keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "  #  keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=1024, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9bc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 16\n",
    "epochs=100\n",
    "#learn_rate=.001\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "#sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "#lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#es = EarlyStopping(monitor='accuracy', mode='max', min_delta=5)\n",
    "#cb = Callback(es)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "#cb_list = [cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a26bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "learn_rate=.001\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist = model.fit(x_train,y_train, validation_data=(x_test, y_test),verbose=1, epochs=100, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)\n",
    "\n",
    "#Fitting the augmentation defined above to the data\n",
    "train_generator.fit(x_train)\n",
    "test_generator.fit(x_test)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(y_train)\n",
    "y_test1=to_categorical(y_test)\n",
    "print(x_train.shape,y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "#import efficientnet.tfkeras as efn\n",
    "print(x_train.shape,y_train1.shape)\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "base_model = EfficientNetB5(include_top=False, weights=\"imagenet\", input_shape=(128 , 128, 3 ),classes= y_train1)\n",
    "model= Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation=('relu'),input_dim=128))\n",
    "model.add(Dense(128,activation=('relu')))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(7,activation=('softmax')))\n",
    "\n",
    "#Adding the Dense layers along with activation and batch normalization\n",
    "model.summary()\n",
    "#Defining the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cc6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 8\n",
    "epochs=100\n",
    "learn_rate=.001\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787119bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train1[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history=model.fit_generator(train_generator.flow(x_train,y_train1, batch_size = batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 2, callbacks = [es],  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef98cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 128, 128, 3) (115, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras_preprocessing/image/image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (115, 128, 128, 3) (128 channels).\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras_preprocessing/image/image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (29, 128, 128, 3) (128 channels).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)\n",
    "\n",
    "#Fitting the augmentation defined above to the data\n",
    "train_generator.fit(x_train)\n",
    "test_generator.fit(x_test)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(y_train)\n",
    "y_test1=to_categorical(y_test)\n",
    "print(x_train.shape,y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96379671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 22:08:47.433372: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-27 22:08:47.433440: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-16-2-55.ap-south-1.compute.internal): /proc/driver/nvidia/version does not exist\n",
      "2022-11-27 22:08:47.434063: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "base_model = MobileNetV3Large(include_top=False, weights=None, input_shape=(128, 128, 3), alpha=1.0, classes=y_train1.shape[1])\n",
    "base_model.load_weights('weights_mobilenet_v3_large_224_1.0_float_no_top.h5') # give the path for downloaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f2bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3large (Functiona  (None, 1, 1, 1280)       4226432   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1311744   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,235,911\n",
      "Trainable params: 6,211,511\n",
      "Non-trainable params: 24,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import efficientnet.tfkeras as efn\n",
    "#print(x_train.shape,y_train1.shape)\n",
    "#from keras import backend as K\n",
    "#K.set_image_data_format('channels_last')\n",
    "#base_model = MobileNetV3Large(include_top=False, weights=\"imagenet\", input_shape=(64 , 64, 3 ),classes=y_train1.shape[1])\n",
    "model= Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "model.add(Dense(512,activation=('relu')))\n",
    "model.add(Dense(256,activation=('relu')))\n",
    "model.add(Dense(128,activation=('relu')))\n",
    "model.add(Dense(64,activation=('relu')))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(7,activation=('softmax')))\n",
    "\n",
    "#Adding the Dense layers along with activation and batch normalization\n",
    "model.summary()\n",
    "#Defining the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48fd952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "batch_size= 16\n",
    "epochs=100\n",
    "learn_rate=.001\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a735d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be51196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras_preprocessing/image/numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3, or 4 channels on axis 1. However, it was passed an array with shape (115, 128, 128, 3) (128 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras_preprocessing/image/numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3, or 4 channels on axis 1. However, it was passed an array with shape (29, 128, 128, 3) (128 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "/tmp/ipykernel_17899/345120107.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=model.fit_generator(train_generator.flow(x_train, y_train1, batch_size = batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 1, callbacks = [es],  verbose = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 11s 761ms/step - loss: 2.0518 - accuracy: 0.1515 - val_loss: 2.0696 - val_accuracy: 0.1875\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 4s 562ms/step - loss: 2.0046 - accuracy: 0.1414 - val_loss: 1.9510 - val_accuracy: 0.3125\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 4s 568ms/step - loss: 1.9427 - accuracy: 0.1515 - val_loss: 1.9099 - val_accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 4s 565ms/step - loss: 1.9093 - accuracy: 0.2121 - val_loss: 1.8426 - val_accuracy: 0.3125\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 4s 639ms/step - loss: 1.8666 - accuracy: 0.1919 - val_loss: 1.8018 - val_accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 4s 566ms/step - loss: 1.8419 - accuracy: 0.2525 - val_loss: 1.7877 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 4s 573ms/step - loss: 1.8247 - accuracy: 0.2626 - val_loss: 1.7105 - val_accuracy: 0.3125\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 5s 641ms/step - loss: 1.8073 - accuracy: 0.2054 - val_loss: 1.6618 - val_accuracy: 0.2500\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 4s 586ms/step - loss: 1.7540 - accuracy: 0.3838 - val_loss: 1.6977 - val_accuracy: 0.1875\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 4s 585ms/step - loss: 1.7356 - accuracy: 0.3434 - val_loss: 1.6594 - val_accuracy: 0.1875\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 5s 648ms/step - loss: 1.7187 - accuracy: 0.4018 - val_loss: 1.6414 - val_accuracy: 0.2500\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 4s 576ms/step - loss: 1.6869 - accuracy: 0.3535 - val_loss: 1.6749 - val_accuracy: 0.3125\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 4s 569ms/step - loss: 1.6641 - accuracy: 0.4040 - val_loss: 1.5187 - val_accuracy: 0.2500\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 4s 561ms/step - loss: 1.6480 - accuracy: 0.3737 - val_loss: 1.4580 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 4s 630ms/step - loss: 1.6286 - accuracy: 0.4107 - val_loss: 1.5456 - val_accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 4s 631ms/step - loss: 1.5966 - accuracy: 0.4018 - val_loss: 1.5231 - val_accuracy: 0.3125\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 4s 566ms/step - loss: 1.5852 - accuracy: 0.4242 - val_loss: 1.5642 - val_accuracy: 0.3750\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 4s 564ms/step - loss: 1.5458 - accuracy: 0.4040 - val_loss: 1.3368 - val_accuracy: 0.3750\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 4s 557ms/step - loss: 1.5209 - accuracy: 0.4040 - val_loss: 1.3536 - val_accuracy: 0.3750\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 4s 571ms/step - loss: 1.5295 - accuracy: 0.4343 - val_loss: 1.3746 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 4s 641ms/step - loss: 1.4841 - accuracy: 0.4949 - val_loss: 1.5157 - val_accuracy: 0.3125\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 4s 561ms/step - loss: 1.4741 - accuracy: 0.5253 - val_loss: 1.5634 - val_accuracy: 0.2500\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 4s 627ms/step - loss: 1.3698 - accuracy: 0.5089 - val_loss: 1.4268 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 4s 627ms/step - loss: 1.3311 - accuracy: 0.4911 - val_loss: 1.5963 - val_accuracy: 0.2500\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 4s 636ms/step - loss: 1.3112 - accuracy: 0.5152 - val_loss: 1.5355 - val_accuracy: 0.3125\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 4s 630ms/step - loss: 1.2132 - accuracy: 0.5556 - val_loss: 1.5079 - val_accuracy: 0.3750\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 4s 563ms/step - loss: 1.2400 - accuracy: 0.5556 - val_loss: 1.3911 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 4s 554ms/step - loss: 1.2351 - accuracy: 0.5253 - val_loss: 1.5246 - val_accuracy: 0.3125\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 4s 624ms/step - loss: 1.1128 - accuracy: 0.5893 - val_loss: 1.5461 - val_accuracy: 0.3750\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 4s 560ms/step - loss: 1.1468 - accuracy: 0.5556 - val_loss: 1.5632 - val_accuracy: 0.3125\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 4s 562ms/step - loss: 1.0745 - accuracy: 0.5859 - val_loss: 1.5637 - val_accuracy: 0.2500\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 4s 635ms/step - loss: 0.9975 - accuracy: 0.6566 - val_loss: 1.6574 - val_accuracy: 0.3125\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 5s 690ms/step - loss: 0.9861 - accuracy: 0.6162 - val_loss: 1.6909 - val_accuracy: 0.3125\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 4s 560ms/step - loss: 0.9587 - accuracy: 0.6263 - val_loss: 1.6046 - val_accuracy: 0.4375\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 4s 556ms/step - loss: 0.9778 - accuracy: 0.5960 - val_loss: 1.4865 - val_accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 4s 624ms/step - loss: 0.8532 - accuracy: 0.6786 - val_loss: 1.5691 - val_accuracy: 0.3750\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 4s 569ms/step - loss: 0.8672 - accuracy: 0.6768 - val_loss: 1.6211 - val_accuracy: 0.4375\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 4s 561ms/step - loss: 0.8091 - accuracy: 0.7071 - val_loss: 1.6895 - val_accuracy: 0.3750\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 4s 564ms/step - loss: 0.9137 - accuracy: 0.6869 - val_loss: 1.8680 - val_accuracy: 0.3750\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 4s 629ms/step - loss: 0.9798 - accuracy: 0.6263 - val_loss: 1.7981 - val_accuracy: 0.3750\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 7s 982ms/step - loss: 0.8645 - accuracy: 0.6970 - val_loss: 1.9610 - val_accuracy: 0.3125\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.8949 - accuracy: 0.6566 - val_loss: 2.0545 - val_accuracy: 0.3125\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 24s 3s/step - loss: 0.7770 - accuracy: 0.6696 - val_loss: 1.9787 - val_accuracy: 0.3125\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.7386 - accuracy: 0.6970 - val_loss: 1.9050 - val_accuracy: 0.3750\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.7004 - accuracy: 0.7879 - val_loss: 1.9851 - val_accuracy: 0.3750\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 4s 579ms/step - loss: 0.6868 - accuracy: 0.7677 - val_loss: 2.3331 - val_accuracy: 0.1250\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 5s 762ms/step - loss: 0.7201 - accuracy: 0.7879 - val_loss: 1.9256 - val_accuracy: 0.3750\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 4s 585ms/step - loss: 0.8109 - accuracy: 0.7273 - val_loss: 1.9810 - val_accuracy: 0.2500\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 4s 571ms/step - loss: 0.6759 - accuracy: 0.8283 - val_loss: 1.7778 - val_accuracy: 0.4375\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 4s 573ms/step - loss: 0.6450 - accuracy: 0.7778 - val_loss: 1.7098 - val_accuracy: 0.4375\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 5s 641ms/step - loss: 0.5497 - accuracy: 0.8839 - val_loss: 1.8209 - val_accuracy: 0.4375\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 4s 571ms/step - loss: 0.4390 - accuracy: 0.8990 - val_loss: 2.0254 - val_accuracy: 0.3125\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 4s 638ms/step - loss: 0.5342 - accuracy: 0.8081 - val_loss: 2.0162 - val_accuracy: 0.3125\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 4s 601ms/step - loss: 0.4210 - accuracy: 0.8990 - val_loss: 2.1148 - val_accuracy: 0.3750\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 6s 999ms/step - loss: 0.4638 - accuracy: 0.8586 - val_loss: 2.3225 - val_accuracy: 0.3125\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.3214 - accuracy: 0.9394 - val_loss: 1.9363 - val_accuracy: 0.3750\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.2997 - accuracy: 0.9596 - val_loss: 1.5672 - val_accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.3553 - accuracy: 0.8788 - val_loss: 1.9617 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.2302 - accuracy: 0.9596 - val_loss: 2.3548 - val_accuracy: 0.2500\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2787 - accuracy: 0.9192 - val_loss: 1.8575 - val_accuracy: 0.1875\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.3045 - accuracy: 0.8990 - val_loss: 1.7597 - val_accuracy: 0.3750\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 23s 3s/step - loss: 0.2090 - accuracy: 0.9464 - val_loss: 2.2085 - val_accuracy: 0.1250\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.1633 - accuracy: 0.9697 - val_loss: 2.0045 - val_accuracy: 0.1875\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 4s 588ms/step - loss: 0.1717 - accuracy: 0.9394 - val_loss: 1.9269 - val_accuracy: 0.3125\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 4s 578ms/step - loss: 0.1845 - accuracy: 0.9495 - val_loss: 1.9101 - val_accuracy: 0.3125\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 4s 585ms/step - loss: 0.1463 - accuracy: 0.9798 - val_loss: 2.2512 - val_accuracy: 0.1875\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 4s 585ms/step - loss: 0.1504 - accuracy: 0.9899 - val_loss: 2.3275 - val_accuracy: 0.3125\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 4s 581ms/step - loss: 0.0951 - accuracy: 0.9697 - val_loss: 2.2168 - val_accuracy: 0.2500\n",
      "Epoch 00068: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_generator.flow(x_train, y_train1, batch_size = batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 1, callbacks = [es],  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)\n",
    "\n",
    "#Fitting the augmentation defined above to the data\n",
    "train_generator.fit(x_train)\n",
    "test_generator.fit(x_test)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(y_train)\n",
    "y_test1=to_categorical(y_test)\n",
    "print(x_train.shape,y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "#import efficientnet.tfkeras as efn\n",
    "print(x_train.shape,y_train1.shape)\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(64 , 64, 3 ),classes=y_train1.shape[1])\n",
    "model= Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation=('relu'),input_dim=256))\n",
    "model.add(Dense(256,activation=('relu')))\n",
    "model.add(Dense(128,activation=('relu')))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(7,activation=('softmax')))\n",
    "\n",
    "#Adding the Dense layers along with activation and batch normalization\n",
    "model.summary()\n",
    "#Defining the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5994753",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "epochs=100\n",
    "learn_rate=.001\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit_generator(train_generator.flow(x_train, y_train1, batch_size = batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 30, callbacks = [es],  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7675926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.metrics\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=(3,3), strides=(2,2), input_shape=(128,128,3), data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59258dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist = model.fit(x_train,y_train,batch_size= 8,verbose=1,epochs=20)#,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Input, Cropping2D, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "#from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EFFICIENT NET (efnet)\n",
    "import keras.metrics\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=48,kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(256,256,3), data_format='channels_last'),\n",
    " #   keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),data_format='channels_last'),\n",
    "    #keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "    #keras.layers.Conv2D(filters=64,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "  #  keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),data_format='channels_last'),\n",
    "    #keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "    #keras.layers.Conv2D(filters=128,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "  #  keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=192, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "    #keras.layers.Conv2D(filters=256,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "  #  keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "   # keras.layers.Conv2D(filters=512,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "  #  keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "   # keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(2048, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2048, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996829ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=['accuracy'])#,keras.metrics.Precision(class_id=10), keras.metrics.Recall(class_id=10)])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy',keras.metrics.Precision(class_id=10), keras.metrics.Recall(class_id=10)])\n",
    "#model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',keras.metrics.Precision(class_id=10), keras.metrics.Recall(class_id=10)])\n",
    "model.compile(optimizer=tf.optimizers.SGD(lr=0.001),loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001))#, metrics=['accuracy',keras.metrics.Precision(class_id=10)])#, keras.metrics.Recall(class_id=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05214993",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist = model.fit(x_train,y_train,verbose=1,epochs=100)#,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "#import keras.utils\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "keras.utils.plot_model(model, to_file = \"Model.png\",  show_shapes=True,  rankdir='TB',expand_nested= True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78242fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = [np.argmax(element) for element in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification Report:   \\n\", classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066638f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debeb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = [np.argmax(element) for element in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08406629",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EFFICIENT NET (efnet)\n",
    "import keras.metrics\n",
    "model = keras.models.Sequential([\n",
    "    #keras.layers.Conv2D(filters=32,kernel_size=(3,3), strides=(2,2), activation='relu', input_shape=(32,32,3), data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=(3,3), strides=(2,2), input_shape=(32,32,3), data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),data_format='channels_last'),\n",
    "    #keras.layers.MaxPool2D((2,2)),\n",
    "    #keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    #keras.layers.Conv2D(filters=64,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),data_format='channels_last'),\n",
    "    #keras.layers.MaxPool2D((2,2)),\n",
    "   # keras.layers.Conv2D(filters=96, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    #keras.layers.Conv2D(filters=128,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Conv2D(filters=96, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1),  padding=\"same\",data_format='channels_last'),\n",
    "    #keras.layers.Conv2D(filters=256,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Conv2D(filters=128, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\",data_format='channels_last'),\n",
    "   # keras.layers.Conv2D(filters=512,kernel_size=(3,3), activation='relu',padding='same'),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=\"same\",data_format='channels_last'),\n",
    "    keras.layers.LeakyReLU(alpha=0.1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "   # keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),data_format='channels_last'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024, activation='relu'),#kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0dca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['no carbon','very low','low','moderate','high','very high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/train1\")\n",
    "#input_folder_path=os.getcwd()\n",
    "#all= input_folder_path.split('/')\n",
    "filename='combined_all2.csv'\n",
    "print(filename)\n",
    "train_df = pd.read_csv(filename)\n",
    "train_labels1 = train_df['Labels'].values\n",
    "#train_labels1 = to_categorical(train_labels1)\n",
    "print(train_labels1.shape)\n",
    "train_images = (train_df.iloc[:,1:].values).astype('float32')/10000.0\n",
    "train_images = train_images.reshape(944,32,32,3)\n",
    "print(train_images.shape)\n",
    "train_labels1 = train_labels1.reshape(944,32,1)\n",
    "train_labels4= np.zeros(944)\n",
    "for i in range(944):\n",
    "  #  for j in range(6):\n",
    "    if(train_labels1[i,0]  > 0):\n",
    "        train_labels4[i] = train_labels1[i,0]\n",
    "train_labels4=train_labels4.astype('int')\n",
    "print(train_labels4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_labels4.reshape(-1,)\n",
    "print(train_labels4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,  x_test, y_train, y_test = train_test_split(train_images,train_label,random_state=2020,test_size=0.2)\n",
    "#x_train= train_images\n",
    "#y_train =train_label\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af492b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist = model.fit(x_train,y_train,verbose=1,epochs=25)#,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4822aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe622153",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plot-keras-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c989108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import show_history, plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "show_history(plot_hist)\n",
    "plot_history(plot_hist, path=\"standard.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0940132",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = [np.argmax(element) for element in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification Report:   \\n\", classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = [np.argmax(element) for element in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e47ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ac4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pydot\n",
    "!conda install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7204e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.utils.plot_model(model, to_file = \"Model.png\",  show_shapes=True,  rankdir='TB',expand_nested= True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vim custom-script.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!conda install  pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "!sudo apt-get install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydot\n",
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pydot_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09330ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False, weights='imagenet',input_shape=(128,128,3))\n",
    "#model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad143ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_model(model, new_input_shape=(None, 40, 40, 3),custom_objects=None):\n",
    "    # replace input shape of first layer\n",
    "    \n",
    "    config = model.layers[0].get_config()\n",
    "    config['batch_input_shape']=new_input_shape\n",
    "    model._layers[0]=model.layers[0].from_config(config)\n",
    "\n",
    "    # rebuild model architecture by exporting and importing via json\n",
    "    new_model = tensorflow.keras.models.model_from_json(model.to_json(),custom_objects=custom_objects)\n",
    "\n",
    "    # copy weights from old model to new one\n",
    "    for layer in new_model._layers:\n",
    "        try:\n",
    "            layer.set_weights(model.get_layer(name=layer.name).get_weights())\n",
    "            print(\"Loaded layer {}\".format(layer.name))\n",
    "        except:\n",
    "            print(\"Could not transfer weights for layer {}\".format(layer.name))\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = change_model(ResNet50,data_format='channels_last',new_input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import keras\n",
    "from keras.layers import Flatten, Dense\n",
    "#from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "#from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def abastufally():\n",
    "    weights = 'imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_resnet_model = Sequential()\n",
    "\n",
    "pretrained_model_for_demo= tf.keras.applications.ResNet50(include_top=False,\n",
    "\n",
    "                   input_shape=(32,32,3),\n",
    "\n",
    "                   pooling='avg',classes=7,\n",
    "\n",
    "                   weights='imagenet')\n",
    "\n",
    "for each_layer in pretrained_model_for_demo.layers:\n",
    "\n",
    "        each_layer.trainable=False\n",
    "\n",
    "demo_resnet_model.add(pretrained_model_for_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_resnet_model.add(Flatten())\n",
    "\n",
    "demo_resnet_model.add(Dense(512, activation='relu'))\n",
    "\n",
    "demo_resnet_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "demo_resnet_model.compile(optimizer=tf.optimizers.SGD(lr=0.001),loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee439a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = demo_resnet_model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd82101",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_resnet_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plot-keras-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import show_history, plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "show_history(history)\n",
    "plot_history(history, path=\"standard_resnet50.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935047cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Flatten, Dense\n",
    "#from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "#from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def abastufally():\n",
    "    weights = 'imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fa371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.keras as efn\n",
    "#from tensorflow.keras.applications import EfficientNetB0\n",
    "base_model = efn.EfficientNetB0( include_top = False, weights = 'imagenet',input_tensor=Input(shape=(32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d990c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size = [224, 224,3]\n",
    "#new_train_x = x_train.resize((224,224,3))\n",
    "new_train_x = tf.image.resize(x_train, size=(224,224))\n",
    "new_train_x = tf.cast(new_train_x, dtype = tf.float32)\n",
    "print(new_train_x.shape)\n",
    "#ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, size), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn\n",
    "#from keras import backend as K\n",
    "#K.set_image_data_format('channels_first')\n",
    "model = efn.EfficientNetB0(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,input_shape=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=tf.optimizers.SGD(lr=0.001),loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5573535",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.moveaxis(x_train , -1, 1)\n",
    "x.shape\n",
    "print( 'x', x,'x_train_______________',x_train,)\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b933a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.keras as efn\n",
    "demo_eff_model = Sequential()\n",
    "\n",
    "model=efn.EfficientNetB0(include_top=False,\n",
    "\n",
    "                   pooling='avg',classes=7,\n",
    "\n",
    "                   weights='imagenet')\n",
    "\n",
    "for each_layer in model.layers:\n",
    "\n",
    "        each_layer.trainable=False\n",
    "\n",
    "demo_eff_model.add(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ec13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_eff_model.add(Flatten())\n",
    "\n",
    "demo_eff_model.add(Dense(512, activation='relu'))\n",
    "\n",
    "demo_eff_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eae7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo_eff_model.compile(optimizer=tf.optimizers.SGD(lr=0.001),loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#demo_eff_model.compile(optimizer=tf.optimizers.SGD(lr=0.001),loss='mean_squared_error', metrics=['accuracy'])\n",
    "demo_eff_model.compile(optimizer=tf.optimizers.SGD(lr=0.001),loss='cross_entropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.moveaxis(x_train, -1, 1)\n",
    "print(x.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcfa89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = demo_eff_model.fit(x, y_train,  epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_efficientnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)\n",
    "\n",
    "#Fitting the augmentation defined above to the data\n",
    "train_generator.fit(x_train)\n",
    "test_generator.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd242bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(y_train)\n",
    "y_test1=to_categorical(y_test)\n",
    "print(x_train.shape,y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "#import efficientnet.tfkeras as efn\n",
    "print(x_train.shape,y_train1.shape)\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "base_model = EfficientNetB5(include_top=False, weights=\"imagenet\", input_shape=(128 , 128, 3 ),classes=y_train1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten()) \n",
    "\n",
    "#Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Dense layers along with activation and batch normalization\n",
    "model.add(Dense(512,activation=('relu'),input_dim=512))\n",
    "\n",
    "model.add(Dense(256,activation=('relu'))) \n",
    "#model.add(Dropout(.3))\n",
    "model.add(Dense(128,activation=('relu')))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(7,activation=('softmax'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "epochs=100\n",
    "learn_rate=.001\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668eddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit_generator(x_train, y_train, epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 100, callbacks = [lrr],  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#es = EarlyStopping(monitor='accuracy', mode='max', min_delta=5)\n",
    "#cb = Callback(es)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)#, patience=20)\n",
    "#es = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=1)\n",
    "#cb_list = [cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "learn_rate=.001\n",
    "batch_size=32\n",
    "epochs=100\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01068e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_hist = model.fit(x_train,y_train1, validation_data=(x_test, y_test1),verbose=1, epochs=100, callbacks=[es])\n",
    "history1=model.fit_generator(train_generator.flow(x_train, y_train1, batch_size = batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 100,   verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the parameters\n",
    "batch_size= 32\n",
    "epochs=100\n",
    "learn_rate=.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit_generator(train_generator.flow(x_train, y_train1, batch_size = batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 100, callbacks = [lrr],  verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plot-keras-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e20880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import show_history, plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "show_history(history)\n",
    "plot_history(history, path=\"standard_efficient.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plotting the training and validation loss\n",
    "\n",
    "f,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n",
    "\n",
    "#Assigning the first subplot to graph training loss and validation loss\n",
    "ax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\n",
    "ax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n",
    "\n",
    "#Plotting the training accuracy and validation accuracy\n",
    "ax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
    "ax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4387983",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca40191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the augmentation defined above to the data\n",
    "train_generator.fit(x_train)\n",
    "test_generator.fit(x_test)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(y_train)\n",
    "y_test1=to_categorical(y_test)\n",
    "print(x_train,y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import efficientnet.tfkeras as efn\n",
    "print(x_train.shape,y_train1.shape)\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(128 , 128, 3 ),classes=y_train1.shape[1])\n",
    "model= Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten()) \n",
    "\n",
    "#Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Dense layers along with activation and batch normalization\n",
    "model.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "\n",
    "model.add(Dense(512,activation=('relu'))) \n",
    "model.add(Dense(256,activation=('relu'))) \n",
    "#model.add(Dropout(.3))\n",
    "model.add(Dense(128,activation=('relu')))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(7,activation=('softmax')))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dede18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the parameters\n",
    "batch_size= 32\n",
    "epochs=100\n",
    "learn_rate=.001\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1=model.fit(x_train, y_train1, epochs = epochs, callbacks = [lrr],  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b06eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plot-keras-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b183c61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plotting the training and validation loss\n",
    "#plt.subplot(1,2,2)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.tight_layout(pad=2.0)\n",
    "#f1.plot.xlabel('epochs')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax1.set_title('Loss')\n",
    "ax2.set_title('Accuracy')\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "#plt.grid(True)\n",
    "# Set the y axis label of the current axis.\n",
    "#plt.ylabel('Loss')\n",
    "#print(model.history.history['loss'])\n",
    "#print('I am here', model.history.history['loss'][-1])\n",
    "ax1.plot(model.history.history['loss'],color='b',label='Training Loss')\n",
    "#ax1.text('hello',loc='right top') {}\".format(input_var1)\n",
    "ax1.legend(['last value: {:.4f}'.format(model.history.history['loss'][-1])],loc='upper right')\n",
    "#annotate last value on the graph\n",
    "#ax1.annotate('%0.2f' % model.history.history['loss'][-1], xy=(1, model.history.history['loss'][-1]), xytext=(8, 0), \n",
    "          #   xycoords=('axes fraction', 'data'), textcoords='offset points')\n",
    "#ax2.savefig('resnet50_accuracy.png')\n",
    "ax2.plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
    "ax2.legend(['last value: {:.4f}'.format(model.history.history['accuracy'][-1])],loc='upper left')\n",
    "plt.savefig('mobilnetv3large_128_plots.png')\n",
    "#plt.text(.99, .99, 'matplotlib', ha='right', va='top', transform=ax.transAxes)\n",
    "#plt.subplot(2,2,2)\n",
    "#ax1=plt.subplot(2,2,2)\n",
    "#plt.xlabel('epochs')\n",
    "#plt.grid(True)\n",
    "# Set the y axis label of the current axis.\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.plot(model.history.history['accuracy'],color='b')\n",
    "#f,ax=plt.subplots(1,1) #Creates 2 subplots under 1 column\n",
    "#ax[0].plot.xlabel(\"epochs\")\n",
    "#Assigning the first subplot to graph training loss and validation loss\n",
    "#ax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\n",
    "#ax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n",
    "\n",
    "#Plotting the training accuracy and validation accuracy\n",
    "#ax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
    "#ax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.fit(x_train)\n",
    "test_generator.fit(x_test)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(y_train)\n",
    "y_test1=to_categorical(y_test)\n",
    "print(x_train,y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38205b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape,y_train1.shape)\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "base_model = MobileNetV3Large(include_top=False, weights=\"imagenet\", input_shape=(32 , 32, 3 ),classes=y_train1.shape[1])\n",
    "model1= Sequential()\n",
    "model1.add(base_model) \n",
    "model1.add(Flatten()) \n",
    "#Model summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3916cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "\n",
    "model1.add(Dense(512,activation=('relu'))) \n",
    "model1.add(Dense(256,activation=('relu'))) \n",
    "#model.add(Dropout(.3))\n",
    "model1.add(Dense(128,activation=('relu')))\n",
    "#model.add(Dropout(.2))\n",
    "model1.add(Dense(7,activation=('softmax')))\n",
    "\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42815d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "epochs=100\n",
    "learn_rate=.001\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "model1.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1=model1.fit_generator(train_generator.flow(x_train, y_train1, batch_size = batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = test_generator.flow(x_test, y_test1, batch_size = batch_size), validation_steps = 10, callbacks = [lrr],  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9681481",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plot-keras-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe822e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import show_history, plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "show_history(history1)\n",
    "plot_history(history1, path=\"standard_mobilenetV3Large.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa8086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
